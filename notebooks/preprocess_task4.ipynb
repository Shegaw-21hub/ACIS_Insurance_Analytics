{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: D:\\Project\\ACIS_Insurance_Analytics\\data\\clean.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18656\\2783333202.py:29: DtypeWarning: Columns (32,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Loaded 1000098 records with 52 columns\n",
      "\n",
      "==================================================\n",
      "STARTING DATA PREPROCESSING\n",
      "==================================================\n",
      "\n",
      "Available columns:\n",
      "['UnderwrittenCoverID', 'PolicyID', 'TransactionMonth', 'IsVATRegistered', 'Citizenship', 'LegalType', 'Title', 'Language', 'Bank', 'AccountType', 'MaritalStatus', 'Gender', 'Country', 'Province', 'PostalCode', 'MainCrestaZone', 'SubCrestaZone', 'ItemType', 'mmcode', 'VehicleType', 'RegistrationYear', 'make', 'Model', 'Cylinders', 'cubiccapacity', 'kilowatts', 'bodytype', 'NumberOfDoors', 'VehicleIntroDate', 'CustomValueEstimate', 'AlarmImmobiliser', 'TrackingDevice', 'CapitalOutstanding', 'NewVehicle', 'WrittenOff', 'Rebuilt', 'Converted', 'CrossBorder', 'NumberOfVehiclesInFleet', 'SumInsured', 'TermFrequency', 'CalculatedPremiumPerTerm', 'ExcessSelected', 'CoverCategory', 'CoverType', 'CoverGroup', 'Section', 'Product', 'StatutoryClass', 'StatutoryRiskType', 'TotalPremium', 'TotalClaims']\n",
      "\n",
      "Using features: ['Province', 'PostalCode', 'Gender', 'VehicleType', 'RegistrationYear', 'SumInsured', 'ExcessSelected', 'CoverType']\n",
      "\n",
      "Found 2788 records with claims (original: 1000098)\n",
      "\n",
      "Data split complete:\n",
      "- Training set: 1951 records\n",
      "- Test set: 837 records\n",
      "\n",
      "Feature types identified:\n",
      "- Numeric: ['PostalCode', 'RegistrationYear', 'SumInsured']\n",
      "- Categorical: ['Province', 'Gender', 'ExcessSelected', 'CoverType', 'VehicleType']\n",
      "\n",
      "Preprocessor configured successfully!\n",
      "\n",
      "==================================================\n",
      "SAVING OUTPUT FILES\n",
      "==================================================\n",
      "- Saved preprocessor.joblib\n",
      "- Saved X_train.csv\n",
      "- Saved X_test.csv\n",
      "- Saved y_train.csv\n",
      "- Saved y_test.csv\n",
      "\n",
      "All outputs saved to 'output' directory\n",
      "\n",
      "==================================================\n",
      "PREPROCESSING COMPLETED SUCCESSFULLY!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. DATA LOADING ==============================================================\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load data with comprehensive error handling\"\"\"\n",
    "    try:\n",
    "        # Convert to Path object and verify existence\n",
    "        data_path = Path(filepath)\n",
    "        if not data_path.exists():\n",
    "            available_files = \"\\n\".join(sorted(data_path.parent.glob(\"*\")))\n",
    "            raise FileNotFoundError(\n",
    "                f\"Data file not found at: {data_path}\\n\"\n",
    "                f\"Available files in directory:\\n{available_files}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"Loading data from: {data_path}\")\n",
    "        \n",
    "        # Try reading the file\n",
    "        df = pd.read_csv(data_path)\n",
    "        print(f\"Success! Loaded {len(df)} records with {len(df.columns)} columns\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ERROR LOADING DATA:\", str(e))\n",
    "        print(\"=\"*50)\n",
    "        print(\"\\nTROUBLESHOOTING GUIDE:\")\n",
    "        print(f\"1. Confirm the file exists at: {data_path}\")\n",
    "        print(f\"2. Check file permissions (try opening it manually)\")\n",
    "        print(f\"3. Verify file is CSV format (not Excel or other format)\")\n",
    "        print(f\"4. Current working directory: {os.getcwd()}\")\n",
    "        if data_path.parent.exists():\n",
    "            print(f\"\\nFiles in data directory:\\n{os.listdir(data_path.parent)}\")\n",
    "        raise\n",
    "\n",
    "# 2. DATA PREPROCESSING =======================================================\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Main preprocessing pipeline\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STARTING DATA PREPROCESSING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Display available columns\n",
    "    print(\"\\nAvailable columns:\")\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "    # Define features to use - MODIFY THESE BASED ON YOUR ACTUAL COLUMNS\n",
    "    features = [\n",
    "        'Province', 'PostalCode', 'Gender', 'VehicleType', 'Make',\n",
    "        'RegistrationYear', 'SumInsured', 'ExcessSelected', 'CoverType'\n",
    "    ]\n",
    "    \n",
    "    # Filter to only keep columns that exist\n",
    "    features = [col for col in features if col in df.columns]\n",
    "    print(\"\\nUsing features:\", features)\n",
    "    \n",
    "    # Filter only policies with claims\n",
    "    if 'TotalClaims' not in df.columns:\n",
    "        raise KeyError(\"'TotalClaims' column not found - required for analysis\")\n",
    "    \n",
    "    claim_data = df[df['TotalClaims'] > 0].copy()\n",
    "    print(f\"\\nFound {len(claim_data)} records with claims (original: {len(df)})\")\n",
    "    \n",
    "    if len(claim_data) == 0:\n",
    "        raise ValueError(\"No claims data available for modeling\")\n",
    "    \n",
    "    # Split data\n",
    "    X = claim_data[features]\n",
    "    y = claim_data['TotalClaims']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    print(\"\\nData split complete:\")\n",
    "    print(f\"- Training set: {len(X_train)} records\")\n",
    "    print(f\"- Test set: {len(X_test)} records\")\n",
    "    \n",
    "    # Define numeric and categorical features\n",
    "    numeric_features = [col for col in features if pd.api.types.is_numeric_dtype(X[col])]\n",
    "    categorical_features = list(set(features) - set(numeric_features))\n",
    "    \n",
    "    print(\"\\nFeature types identified:\")\n",
    "    print(f\"- Numeric: {numeric_features}\")\n",
    "    print(f\"- Categorical: {categorical_features}\")\n",
    "    \n",
    "    # Create preprocessing pipelines\n",
    "    numeric_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nPreprocessor configured successfully!\")\n",
    "    return preprocessor, X_train, X_test, y_train, y_test\n",
    "\n",
    "# 3. SAVE OUTPUTS =============================================================\n",
    "\n",
    "def save_outputs(preprocessor, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Save all preprocessing artifacts\"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = Path(\"output\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SAVING OUTPUT FILES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Save artifacts\n",
    "    artifacts = {\n",
    "        'preprocessor.joblib': preprocessor,\n",
    "        'X_train.csv': X_train,\n",
    "        'X_test.csv': X_test,\n",
    "        'y_train.csv': y_train,\n",
    "        'y_test.csv': y_test\n",
    "    }\n",
    "    \n",
    "    for filename, obj in artifacts.items():\n",
    "        filepath = output_dir / filename\n",
    "        if isinstance(obj, (pd.DataFrame, pd.Series)):\n",
    "            obj.to_csv(filepath, index=False)\n",
    "        else:\n",
    "            joblib.dump(obj, filepath)\n",
    "        print(f\"- Saved {filename}\")\n",
    "    \n",
    "    print(\"\\nAll outputs saved to 'output' directory\")\n",
    "\n",
    "# MAIN EXECUTION ==============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # 1. Configure paths - UPDATE THIS TO YOUR ACTUAL PATH\n",
    "        filepath = \"D:/Project/ACIS_Insurance_Analytics/data/clean.csv\"\n",
    "        \n",
    "        # 2. Load data\n",
    "        df = load_data(filepath)\n",
    "        \n",
    "        # 3. Preprocess data\n",
    "        preprocessor, X_train, X_test, y_train, y_test = preprocess_data(df)\n",
    "        \n",
    "        # 4. Save outputs\n",
    "        save_outputs(preprocessor, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"PROCESS FAILED:\", str(e))\n",
    "        print(\"=\"*50)\n",
    "        print(\"\\nNext steps:\")\n",
    "        print(\"1. Check the error message above\")\n",
    "        print(\"2. Verify your data file format and contents\")\n",
    "        print(\"3. Ensure all required columns are present\")\n",
    "        print(\"4. Check file permissions and paths\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
